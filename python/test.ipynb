{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'true_labels': ['cat', 'dog', 'cat', 'dog', 'cat', 'dog'],\n",
    "    'predicted_label1': ['cat', 'cat', 'dog', 'dog', 'cat', 'cat'],\n",
    "    'predicted_label2': ['dog', 'dog', 'cat', 'cat', 'dog', 'dog'],\n",
    "    'predicted_label3': ['rabbit', 'fish', 'dog', 'cat', 'rabbit', 'fish']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the value of 'k' for top-k accuracy\n",
    "k = 2  # Change this value to the desired 'k'\n",
    "\n",
    "# Combine predicted label columns into a single list of predicted labels\n",
    "df['predicted_labels'] = df.apply(lambda row: [row['predicted_label1'], row['predicted_label2'], row['predicted_label3']], axis=1)\n",
    "\n",
    "# Calculate top-k accuracy\n",
    "df['is_correct'] = df.apply(lambda row: row['true_labels'] in row['predicted_labels'], axis=1)\n",
    "accuracy = accuracy_score(df['true_labels'], df['predicted_labels'])\n",
    "top_k_accuracy = top_k_accuracy_score(df['true_labels'], df['predicted_labels'], k=k)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Top-{k} Accuracy: {top_k_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "# Create a sample DataFrame with actual values in 'actual' column\n",
    "# and predicted values in 'pred1', 'pred2', and 'pred3' columns\n",
    "data = {\n",
    "    'actual': [2, 1, 3, 4, 2],\n",
    "    'pred1': [2, 1, 3, 4, 5],\n",
    "    'pred2': [1, 2, 3, 4, 5],\n",
    "    'pred3': [3, 1, 2, 5, 4]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the value of k for top-k accuracy\n",
    "k = 2\n",
    "\n",
    "# Calculate top-k accuracy for each row\n",
    "def calculate_top_k_accuracy(row):\n",
    "    actual = [row['actual']]\n",
    "    predicted = [row['pred1'], row['pred2'], row['pred3']]\n",
    "    top_k_acc = top_k_accuracy_score([actual], [predicted], k=k)\n",
    "    return top_k_acc\n",
    "\n",
    "df['top_k_accuracy'] = df.apply(calculate_top_k_accuracy, axis=1)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "# Create a sample DataFrame with actual values in 'actual' column\n",
    "# and predicted values in 'pred1', 'pred2', and 'pred3' columns\n",
    "data = {\n",
    "    'actual': [2, 1, 3, 4, 2],\n",
    "    'pred1': [2, 1, 3, 4, 5],\n",
    "    'pred2': [1, 2, 3, 4, 5],\n",
    "    'pred3': [3, 1, 2, 5, 4]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the value of k for top-k accuracy\n",
    "k = 2\n",
    "\n",
    "# Extract actual values and predicted values from the DataFrame\n",
    "actual_values = df['actual'].values\n",
    "predicted_values = df[['pred1', 'pred2', 'pred3']].values\n",
    "\n",
    "# Calculate top-k accuracy for the entire DataFrame\n",
    "top_k_acc = top_k_accuracy_score(actual_values.reshape(-1, 1), predicted_values, k=k)\n",
    "\n",
    "print(f'Top-{k} Accuracy: {top_k_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes in 'y_true' (4) not equal to the number of classes in 'y_score' (3).You can provide a list of all known classes by assigning it to the `labels` parameter.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m predicted_values \u001b[39m=\u001b[39m df[[\u001b[39m'\u001b[39m\u001b[39mpred1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpred2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpred3\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m     22\u001b[0m \u001b[39m# Calculate top-k accuracy for the entire DataFrame\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m top_k_acc \u001b[39m=\u001b[39m top_k_accuracy_score(actual_values, predicted_values, k\u001b[39m=\u001b[39;49mk)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTop-\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtop_k_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_ranking.py:1774\u001b[0m, in \u001b[0;36mtop_k_accuracy_score\u001b[1;34m(y_true, y_score, k, normalize, sample_weight, labels)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     n_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(classes)\n\u001b[0;32m   1773\u001b[0m     \u001b[39mif\u001b[39;00m n_classes \u001b[39m!=\u001b[39m y_score_n_classes:\n\u001b[1;32m-> 1774\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1775\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of classes in \u001b[39m\u001b[39m'\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mn_classes\u001b[39m}\u001b[39;00m\u001b[39m) not equal \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1776\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto the number of classes in \u001b[39m\u001b[39m'\u001b[39m\u001b[39my_score\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00my_score_n_classes\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1777\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou can provide a list of all known classes by assigning it \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1778\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mto the `labels` parameter.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1779\u001b[0m         )\n\u001b[0;32m   1780\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1781\u001b[0m     labels \u001b[39m=\u001b[39m column_or_1d(labels)\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes in 'y_true' (4) not equal to the number of classes in 'y_score' (3).You can provide a list of all known classes by assigning it to the `labels` parameter."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "# Create a sample DataFrame with actual values in 'actual' column\n",
    "# and predicted values in 'pred1', 'pred2', and 'pred3' columns\n",
    "data = {\n",
    "    'actual': [2, 1, 3, 4, 2],\n",
    "    'pred1': [2, 1, 3, 4, 5],\n",
    "    'pred2': [1, 2, 3, 4, 5],\n",
    "    'pred3': [3, 1, 2, 5, 4]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the value of k for top-k accuracy\n",
    "k = 2\n",
    "\n",
    "# Extract actual values and predicted values from the DataFrame\n",
    "actual_values = df['actual'].values.reshape(-1, 1)\n",
    "predicted_values = df[['pred1', 'pred2', 'pred3']].values\n",
    "\n",
    "# Calculate top-k accuracy for the entire DataFrame\n",
    "top_k_acc = top_k_accuracy_score(actual_values, predicted_values, k=k)\n",
    "\n",
    "print(f'Top-{k} Accuracy: {top_k_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3  # Define the value of k for top-k accuracy\n",
    "\n",
    "# Create a function to calculate top-k accuracy\n",
    "def top_k_accuracy(row):\n",
    "    actual = row['Actual_Label']\n",
    "    top_k_preds = [row['Pred_Label_1'], row['Pred_Label_2'], row['Pred_Label_3']]  # Adjust as per your number of predictions\n",
    "\n",
    "    return actual in top_k_preds[:k]\n",
    "\n",
    "# Apply the top_k_accuracy function to each row and calculate accuracy\n",
    "df['Top_K_Accuracy'] = df.apply(top_k_accuracy, axis=1)\n",
    "\n",
    "# Calculate the overall top-k accuracy\n",
    "overall_top_k_accuracy = df['Top_K_Accuracy'].mean()\n",
    "\n",
    "print(f\"Top-{k} Accuracy: {overall_top_k_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
